{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df553600-bf41-4158-92fb-fc355a9be5b3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec84c0a1-5a07-40be-95b1-df1dec36c39e",
   "metadata": {},
   "source": [
    "# Dissecting the Data Mesh technical platform : `polyexpose` end to end notebook demo\n",
    "\n",
    "## Overview\n",
    "This demo showcases and end to end example of runnig `polyexpose` for exposing a dataset via different interfaces (polyglot data):\n",
    "* SQL\n",
    "* GQL\n",
    "* Events\n",
    "* Plain text file (CSV)\n",
    "\n",
    "### Set up your local development environment\n",
    "\n",
    "If you are  Google Cloud Notebooks, your environment already meets all the requirements to run this notebook. You can skip this step.\n",
    "\n",
    "Otherwise, make sure your environment meets this notebook's requirements. You need the following:\n",
    "\n",
    "* The Google Cloud SDK\n",
    "* Python 3\n",
    "* virtualenv\n",
    "* Jupyter notebook running in a virtual environment with Python 3\n",
    "\n",
    "The Google Cloud guide to Setting up a Python development environment and the Jupyter installation guide provide detailed instructions for meeting these requirements. \n",
    "\n",
    "\n",
    "### Set up `polyexpose` infraestructure on GCP\n",
    "\n",
    "Prior executing this notebook, some infra. needs to be deployed on GCP, in particular:\n",
    "\n",
    "* A SPARK cluster (dataproc)\n",
    "* hasura GQL engine running on GKE + Cloud SQL database for metadata\n",
    "* GCS buckets\n",
    "* Service accounts\n",
    "\n",
    "The easiest way to deploy the infraestructure is running the bootstrap script from the terminal in a Google Cloud Notebooks.\n",
    "\n",
    "In order to deploy the infra:\n",
    "\n",
    "* Generate a `config.yaml` file with the following structure, or just edit the supplied template:\n",
    "```yaml\n",
    "bigquery:\n",
    "  #BigQuery dataset name for hasura staging\n",
    "  dataset: <TO_DO_DEVELOPER>\n",
    "  #BigQuery dataset location (e.g. US) for hasura staging\n",
    "  dataset_location: <TO_DO_DEVELOPER>\n",
    "gcp:\n",
    "  #Google Cloud Project ID\n",
    "  gcp_project_id: <TO_DO_DEVELOPER>\n",
    "  #Google Project number\n",
    "  gcp_project_number: <TO_DO_DEVELOPER>\n",
    "  #Google default region\n",
    "  gcp_region: <TO_DO_DEVELOPER>\n",
    "  #Google default zone\n",
    "  gcp_zone: <TO_DO_DEVELOPER>\n",
    "  #General Service Account name, it needs enough privigies to create/edit resources .. (e.g. Project Owner)\n",
    "  sa_name : <TO_DO_DEVELOPER>\n",
    "  #General Service Account name key location where the JSON file will be generated\n",
    "  sa_key: <TO_DO_DEVELOPER>\n",
    "  #Terrafrom Service Account name, it needs enough privigies to create/edit resources ..\n",
    "  sa_terraform_name : <TO_DO_DEVELOPER>\n",
    "  #Terraform Service Account name key location where the JSON file will be generated\n",
    "  sa_terraform_key : <TO_DO_DEVELOPER>\n",
    "gke:\n",
    "   #GKE cluster name where graphQL engine will be deployed\n",
    "   gke_cluster_name : <TO_DO_DEVELOPER>\n",
    "   #hasura needs a metadata database, this will be the prefix name of a Postgres 9.6 database\n",
    "   dbname_prefix: <TO_DO_DEVELOPER>\n",
    "gcs:\n",
    "  #GCS bucket location\n",
    "  bucket_location: <TO_DO_DEVELOPER>\n",
    "  #GCS bucket name\n",
    "  bucket_name: <TO_DO_DEVELOPER>\n",
    "  #GCS path where sample data will be deployed (e.g. /stg_parquet/ )\n",
    "  sample_data_folder: <TO_DO_DEVELOPER>\n",
    "  #GCS path where the SPARK templates will be deployed (e.g. /scripts/ )\n",
    "  scripts_folder: <TO_DO_DEVELOPER>\n",
    "  #GCS path Iceberg SPARK catalog (e.g. /warehouse/)\n",
    "  #TODO (velascoluis) : Add support for metastore service\n",
    "  warehouse_dir_folder: <TO_DO_DEVELOPER>\n",
    "hasura:\n",
    "  #Leave this filed unchanged for now\n",
    "  url: <UNKNOWN>\n",
    "pubsublite:\n",
    "  #PubSubLite Topic region, the name is autogenerated from the base table\n",
    "  pubsublite_gcp_region: <TO_DO_DEVELOPER>\n",
    "  #This is the time the stream service will be running in seconds (e.g. 3600) During this time the dataproc SPARK cluster will be up listening for changes on the base Iceberg table\n",
    "  stream_time: <TO_DO_DEVELOPER>\n",
    "  #This is the offset time in miliseconds on where to look for table changes. It can 0 to listen for new changes or can be negative (e.g. -1000000000), to start generating events from that point in the past\n",
    "  timestamp_offset_mili: <TO_DO_DEVELOPER>\n",
    "spark:\n",
    "  #SPARK cluster name (single node cluster for now)\n",
    "  dataproc_cluster_name: <TO_DO_DEVELOPER>\n",
    "  #SPARK dataproc GCP region\n",
    "  dataproc_gcp_region: <TO_DO_DEVELOPER>\n",
    "  spark_templates:\n",
    "  #Do not edit these fields\n",
    "  - iceberg_show_catalog: show_catalog.py\n",
    "  - iceberg_exec_sql: exec_sql.py\n",
    "  - iceberg_export_csv: export_csv.py\n",
    "  - iceberg_export_bigquery: export_bigquery.py\n",
    "  - iceberg_stream_pubsub: stream_pubsub.py\n",
    "  - create_sample_table: create_sample_table.py\n",
    "```\n",
    "\n",
    "If you need more control on the infra (e.g. dataproc/gke cluster number of nodes), feel free to edit the terraform script `main.tf`. The default values are cost conscious.\n",
    "* Start a shell terminal and execute the `bootstrap_polyexpose_infra.sh config.yaml` script\n",
    "\n",
    "\n",
    "### Install `polyexpose` package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a55e359-dce1-4387-99c6-dd47f12ed6b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: polyexpose==0.0.5 in /opt/conda/lib/python3.7/site-packages (0.0.5)\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.7/site-packages (from polyexpose==0.0.5) (3.20.1)\n",
      "Requirement already satisfied: pyspark~=3.3.0 in /opt/conda/lib/python3.7/site-packages (from polyexpose==0.0.5) (3.3.0)\n",
      "Requirement already satisfied: google-cloud-dataproc in /opt/conda/lib/python3.7/site-packages (from polyexpose==0.0.5) (4.0.3)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from polyexpose==0.0.5) (6.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from polyexpose==0.0.5) (2.28.1)\n",
      "Requirement already satisfied: google-cloud-storage in /opt/conda/lib/python3.7/site-packages (from polyexpose==0.0.5) (2.4.0)\n",
      "Requirement already satisfied: google-cloud-pubsublite==1.4.2 in /opt/conda/lib/python3.7/site-packages (from polyexpose==0.0.5) (1.4.2)\n",
      "Requirement already satisfied: google-cloud-pubsub<3.0.0dev,>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-pubsublite==1.4.2->polyexpose==0.0.5) (2.13.1)\n",
      "Requirement already satisfied: grpcio-status>=1.18.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-pubsublite==1.4.2->polyexpose==0.0.5) (1.47.0)\n",
      "Requirement already satisfied: overrides<7.0.0,>=6.0.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-pubsublite==1.4.2->polyexpose==0.0.5) (6.1.0)\n",
      "Requirement already satisfied: grpcio>=1.18.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-pubsublite==1.4.2->polyexpose==0.0.5) (1.47.0)\n",
      "Requirement already satisfied: py4j==0.10.9.5 in /opt/conda/lib/python3.7/site-packages (from pyspark~=3.3.0->polyexpose==0.0.5) (0.10.9.5)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-dataproc->polyexpose==0.0.5) (1.20.6)\n",
      "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /opt/conda/lib/python3.7/site-packages (from google-cloud-dataproc->polyexpose==0.0.5) (2.8.1)\n",
      "Requirement already satisfied: google-resumable-media>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage->polyexpose==0.0.5) (2.3.3)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage->polyexpose==0.0.5) (2.3.1)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage->polyexpose==0.0.5) (2.9.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->polyexpose==0.0.5) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->polyexpose==0.0.5) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->polyexpose==0.0.5) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->polyexpose==0.0.5) (2022.6.15)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-dataproc->polyexpose==0.0.5) (1.56.3)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage->polyexpose==0.0.5) (1.16.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage->polyexpose==0.0.5) (5.0.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage->polyexpose==0.0.5) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage->polyexpose==0.0.5) (0.2.7)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /opt/conda/lib/python3.7/site-packages (from google-cloud-pubsub<3.0.0dev,>=2.1.0->google-cloud-pubsublite==1.4.2->polyexpose==0.0.5) (0.12.4)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media>=2.3.2->google-cloud-storage->polyexpose==0.0.5) (1.1.2)\n",
      "Requirement already satisfied: typing-utils>=0.0.3 in /opt/conda/lib/python3.7/site-packages (from overrides<7.0.0,>=6.0.1->google-cloud-pubsublite==1.4.2->polyexpose==0.0.5) (0.1.0)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media>=2.3.2->google-cloud-storage->polyexpose==0.0.5) (1.15.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage->polyexpose==0.0.5) (0.4.8)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media>=2.3.2->google-cloud-storage->polyexpose==0.0.5) (2.21)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install polyexpose==0.0.5\n",
    "!pip3 install pyyaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6684f184-2320-4095-8c39-6328725bf249",
   "metadata": {},
   "source": [
    "### Generate configuration for `polyexpose`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fd079f-e71b-417f-9478-084f5ca2d25d",
   "metadata": {},
   "source": [
    "Lets create the basic config file for `polyexpose`. We can basically reuse the config file we used for the boostraping step. We just need to edit and adjust:\n",
    "```yaml\n",
    "gcp:\n",
    "  sa_key: infra_deploy/sa-hasura.json\n",
    "hasura:\n",
    "  url: http://34.94.177.87/\n",
    "```\n",
    "\n",
    "To get the hasura url, just execute:\n",
    "\n",
    "```bash\n",
    "jupyter@python-polyexpose:~/infra_deploy$ kubectl get service\n",
    "NAME         TYPE           CLUSTER-IP     EXTERNAL-IP    PORT(S)        AGE\n",
    "hasura       LoadBalancer   10.3.249.109   34.94.177.87   80:30453/TCP   2m11s\n",
    "kubernetes   ClusterIP      10.3.240.1     <none>         443/TCP        16m\n",
    "```\n",
    "Copy the EXTERNAL-IP field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb100126-d779-404d-80f3-ac3c361cbe4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creates the basic config file for polyexpose in YAML format\n",
    "\"\"\"\n",
    "import yaml\n",
    "CONFIG_FILE_NAME='config.yaml'\n",
    "config_contents = yaml.safe_load(\"\"\"\n",
    "bigquery:\n",
    "  dataset: polyexpose\n",
    "  dataset_location: US\n",
    "gcp:\n",
    "  gcp_project_id: my-gcp-project\n",
    "  gcp_project_number: '1063524325524'\n",
    "  gcp_region: us-west2\n",
    "  gcp_zone: us-west2-a\n",
    "  sa_name : sa-hasura\n",
    "  sa_key: infra_deploy/sa-hasura.json\n",
    "  sa_terraform_name : sa-tf-deployer \n",
    "  sa_terraform_key : infra_deploy/sa-tf-deployer.json\n",
    "gke:\n",
    "   gke_cluster_name : gke-hasura-cluster\n",
    "   dbname_prefix: sql-hasura-database\n",
    "gcs:\n",
    "  bucket_location: US\n",
    "  bucket_name: gcs-my-project-dev-sandbox\n",
    "  sample_data_folder: /stg_parquet/\n",
    "  scripts_folder: /scripts/\n",
    "  warehouse_dir_folder: /warehouse/\n",
    "hasura:\n",
    "  url: http://34.94.177.87/\n",
    "pubsublite:\n",
    "  pubsublite_gcp_region: us-west2\n",
    "  stream_time: '60'\n",
    "  timestamp_offset_mili: '-1000000000'\n",
    "spark:\n",
    "  dataproc_cluster_name: spark-cluster\n",
    "  dataproc_gcp_region: us-west2\n",
    "  spark_templates:\n",
    "  - iceberg_show_catalog: show_catalog.py\n",
    "  - iceberg_exec_sql: exec_sql.py\n",
    "  - iceberg_export_csv: export_csv.py\n",
    "  - iceberg_export_bigquery: export_bigquery.py\n",
    "  - iceberg_stream_pubsub: stream_pubsub.py\n",
    "  - create_sample_table: create_sample_table.py\n",
    "\"\"\")\n",
    "with open(CONFIG_FILE_NAME, 'w') as file:\n",
    "    documents = yaml.dump(config_contents, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2833f9a6-b887-4244-83ad-dfa370066f94",
   "metadata": {},
   "source": [
    "### Basic functionality\n",
    "\n",
    "Generating a client and loading a base table.\n",
    "The basic abstraction of polyexpose are tables, in order to generate one we need to read the internal catalog and create a reference using the `get_table`function.\n",
    "Generating the tables and registering in the catalog are responsability of other componets in the data product, but we provide a simple `create_sample_table` function for testing purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7824a8fd-e974-4041-8282-3937d9b598e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Uncomment for enabling package INFO messages'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Uncomment for enabling package INFO messages\"\"\"\n",
    "#import logging\n",
    "#logging.basicConfig()\n",
    "#logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a584280-6819-4712-a377-364af16e8365",
   "metadata": {},
   "outputs": [],
   "source": [
    "from polyexpose import polyexpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25e1c5c4-dbdc-4344-97b2-3a92b37feee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = polyexpose.PolyExpose(yaml_config=CONFIG_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45fde157-913b-4157-89e1-d7b80b27454d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package initialized!\n"
     ]
    }
   ],
   "source": [
    "client.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ffb39b8-064f-455e-9c16-b87eec76a7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/10 13:41:06 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n",
      "22/07/10 13:41:06 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n",
      "22/07/10 13:41:06 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "22/07/10 13:41:06 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n",
      "22/07/10 13:41:06 INFO org.sparkproject.jetty.util.log: Logging initialized @4092ms to org.sparkproject.jetty.util.log.Slf4jLog\n",
      "22/07/10 13:41:06 INFO org.sparkproject.jetty.server.Server: jetty-9.4.40.v20210413; built: 2021-04-13T20:42:42.668Z; git: b881a572662e1943a14ae12e7e1207989f218b74; jvm 1.8.0_332-b09\n",
      "22/07/10 13:41:06 INFO org.sparkproject.jetty.server.Server: Started @4221ms\n",
      "22/07/10 13:41:06 INFO org.sparkproject.jetty.server.AbstractConnector: Started ServerConnector@2f76deac{HTTP/1.1, (http/1.1)}{0.0.0.0:44591}\n",
      "22/07/10 13:41:07 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at spark-cluster-m/10.168.0.42:8032\n",
      "22/07/10 13:41:07 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at spark-cluster-m/10.168.0.42:10200\n",
      "22/07/10 13:41:08 INFO org.apache.hadoop.conf.Configuration: resource-types.xml not found\n",
      "22/07/10 13:41:08 INFO org.apache.hadoop.yarn.util.resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "22/07/10 13:41:09 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1657456539755_0009\n",
      "22/07/10 13:41:10 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at spark-cluster-m/10.168.0.42:8030\n",
      "22/07/10 13:41:13 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.\n",
      "POLYEXPOSE - Creating sample table: polytable03\n",
      "\n",
      "POLYEXPOSE - Table created \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "client.create_sample_table('polytable03')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "679d9c2c-173e-4ef2-9a36-8c809d1ce680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables avaliable in catalog:['default.polytable03']\n"
     ]
    }
   ],
   "source": [
    "#force_reload bypass a local cache\n",
    "client.load_catalog(force_reload=True)\n",
    "client.show_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "109fe312-bb30-4634-bf15-dc9adde1317a",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = 'default.polytable03'\n",
    "table = client.get_table(table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791a4111-6855-499e-a56d-a47463470f1b",
   "metadata": {},
   "source": [
    "### Expose mode: SQL\n",
    "\n",
    "For each expose mode, we first need to enable it using the `expose_as` function. In the SQL case, we just need to run `expose_as(\"SQL\")`on the desired table. This will unlock executing the `query_table` that takes two parameters : [1] list of projections ids, [2] list of filters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcf28ac9-f8e8-4197-ad2f-50ba27a081ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "table.expose_as(\"SQL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea966432-a769-41fa-97be-5d1b2a38feaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/10 13:42:54 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n",
      "22/07/10 13:42:54 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n",
      "22/07/10 13:42:54 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "22/07/10 13:42:54 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n",
      "22/07/10 13:42:54 INFO org.sparkproject.jetty.util.log: Logging initialized @4026ms to org.sparkproject.jetty.util.log.Slf4jLog\n",
      "22/07/10 13:42:54 INFO org.sparkproject.jetty.server.Server: jetty-9.4.40.v20210413; built: 2021-04-13T20:42:42.668Z; git: b881a572662e1943a14ae12e7e1207989f218b74; jvm 1.8.0_332-b09\n",
      "22/07/10 13:42:54 INFO org.sparkproject.jetty.server.Server: Started @4159ms\n",
      "22/07/10 13:42:54 INFO org.sparkproject.jetty.server.AbstractConnector: Started ServerConnector@7b3210e5{HTTP/1.1, (http/1.1)}{0.0.0.0:33739}\n",
      "22/07/10 13:42:55 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at spark-cluster-m/10.168.0.42:8032\n",
      "22/07/10 13:42:55 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at spark-cluster-m/10.168.0.42:10200\n",
      "22/07/10 13:42:55 INFO org.apache.hadoop.conf.Configuration: resource-types.xml not found\n",
      "22/07/10 13:42:55 INFO org.apache.hadoop.yarn.util.resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "22/07/10 13:42:57 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1657456539755_0011\n",
      "22/07/10 13:42:59 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at spark-cluster-m/10.168.0.42:8030\n",
      "22/07/10 13:43:01 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.\n",
      "POLYEXPOSE - Executing SQL command: \n",
      " DESCRIBE TABLE default.polytable03\n",
      "ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/etc/hive/conf.dist/ivysettings.xml will be used\n",
      "POLYEXPOSE - Command output: \n",
      " +-----------------+---------+-------+\n",
      "|         col_name|data_type|comment|\n",
      "+-----------------+---------+-------+\n",
      "|registration_dttm|timestamp|       |\n",
      "|               id|      int|       |\n",
      "|       first_name|   string|       |\n",
      "|        last_name|   string|       |\n",
      "|            email|   string|       |\n",
      "|           gender|   string|       |\n",
      "|       ip_address|   string|       |\n",
      "|               cc|   string|       |\n",
      "|          country|   string|       |\n",
      "|        birthdate|   string|       |\n",
      "|           salary|   double|       |\n",
      "|            title|   string|       |\n",
      "|         comments|   string|       |\n",
      "|                 |         |       |\n",
      "|   # Partitioning|         |       |\n",
      "|  Not partitioned|         |       |\n",
      "+-----------------+---------+-------+\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table.show_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3aef1e75-f3ad-4806-acc2-e4028df5c098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/10 13:43:28 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n",
      "22/07/10 13:43:28 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n",
      "22/07/10 13:43:28 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "22/07/10 13:43:28 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n",
      "22/07/10 13:43:28 INFO org.sparkproject.jetty.util.log: Logging initialized @4143ms to org.sparkproject.jetty.util.log.Slf4jLog\n",
      "22/07/10 13:43:28 INFO org.sparkproject.jetty.server.Server: jetty-9.4.40.v20210413; built: 2021-04-13T20:42:42.668Z; git: b881a572662e1943a14ae12e7e1207989f218b74; jvm 1.8.0_332-b09\n",
      "22/07/10 13:43:28 INFO org.sparkproject.jetty.server.Server: Started @4265ms\n",
      "22/07/10 13:43:28 INFO org.sparkproject.jetty.server.AbstractConnector: Started ServerConnector@564d22c4{HTTP/1.1, (http/1.1)}{0.0.0.0:43105}\n",
      "22/07/10 13:43:29 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at spark-cluster-m/10.168.0.42:8032\n",
      "22/07/10 13:43:29 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at spark-cluster-m/10.168.0.42:10200\n",
      "22/07/10 13:43:30 INFO org.apache.hadoop.conf.Configuration: resource-types.xml not found\n",
      "22/07/10 13:43:30 INFO org.apache.hadoop.yarn.util.resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "22/07/10 13:43:32 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1657456539755_0012\n",
      "22/07/10 13:43:33 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at spark-cluster-m/10.168.0.42:8030\n",
      "22/07/10 13:43:35 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.\n",
      "POLYEXPOSE - Executing SQL command: \n",
      " SELECT id,first_name,salary,country FROM default.polytable03 WHERE salary>50000\n",
      "ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/etc/hive/conf.dist/ivysettings.xml will be used\n",
      "POLYEXPOSE - Command output: \n",
      " +---+----------+---------+--------------------+\n",
      "| id|first_name|   salary|             country|\n",
      "+---+----------+---------+--------------------+\n",
      "|  2|    Albert|150280.17|              Canada|\n",
      "|  3|    Evelyn|144972.51|              Russia|\n",
      "|  4|    Denise| 90263.05|               China|\n",
      "|  6|   Kathryn| 69227.11|           Indonesia|\n",
      "|  8|     Harry|186469.43|Bosnia and Herzeg...|\n",
      "|  9|      Jose|231067.84|         South Korea|\n",
      "| 11|     Susan|210001.95|              Russia|\n",
      "| 14|     Kathy|286592.99|Bosnia and Herzeg...|\n",
      "| 15|   Dorothy|157099.71|               Japan|\n",
      "| 16|     Bruce|239100.65|              Brazil|\n",
      "| 17|     Emily|116800.65|              Russia|\n",
      "| 18|   Stephen|248877.99|             Ukraine|\n",
      "| 19|  Clarence|177122.99|              Russia|\n",
      "| 20|   Rebecca|137251.19|               China|\n",
      "| 21|     Diane| 87978.22|              Russia|\n",
      "| 22|  Lawrence|131283.64|            Tanzania|\n",
      "| 23|   Gregory|182233.49|             Tunisia|\n",
      "| 24|  Michelle|278001.46|            Tanzania|\n",
      "| 25|    Rachel|176178.75|              Russia|\n",
      "| 26|   Anthony|170085.81|               Japan|\n",
      "| 27|     Henry|284300.15|               China|\n",
      "| 28|    Samuel|108950.24|              Brazil|\n",
      "| 29|Jacqueline|247939.52|       United States|\n",
      "| 30|     Annie|118310.72|             Nigeria|\n",
      "| 31|   Antonio|135007.96|            Thailand|\n",
      "| 32|    Nicole|149720.75|       United States|\n",
      "| 33| Christina|242593.85|              Greece|\n",
      "| 34|  Margaret|109644.23|        South Africa|\n",
      "| 36|     Betty|  91370.3|              France|\n",
      "| 37|   Dorothy| 57194.86|               China|\n",
      "| 38|   Kathryn| 67783.73|      Czech Republic|\n",
      "| 39|      Jose|134708.82|               Chile|\n",
      "| 40|      Jack|  81685.1|           Argentina|\n",
      "| 41|    Walter|212105.33|             Somalia|\n",
      "| 42|      Todd|284728.99|               Japan|\n",
      "| 43|    Amanda|213410.26|               China|\n",
      "| 44|    Sharon|133884.94|              France|\n",
      "| 45|    Bonnie| 67661.42|             Germany|\n",
      "| 46|   Deborah|111569.22|              Canada|\n",
      "| 47|    Daniel| 66260.14|Central African R...|\n",
      "| 48|      Jean|199100.32|               Nepal|\n",
      "| 49|      Lisa|210631.91|             Germany|\n",
      "| 50|      Sean|256068.38|Bosnia and Herzeg...|\n",
      "| 51|    Ernest|100269.36|            Portugal|\n",
      "| 52|    Louise|173300.37|            Ethiopia|\n",
      "| 53|     Ralph| 168208.4|               China|\n",
      "| 54|    George| 153238.6|           Macedonia|\n",
      "| 55|      Anna|  92837.5|               China|\n",
      "| 56|    Cheryl|200827.88|             Finland|\n",
      "| 57|    Willie|184978.64|               China|\n",
      "| 58|    Arthur|144164.88|               China|\n",
      "| 59|  Patricia| 69236.54|               China|\n",
      "| 60|   Cynthia| 179378.0|              Brazil|\n",
      "| 61|     David|197445.45|              Mexico|\n",
      "| 62|     Julia|118311.39|             Bolivia|\n",
      "| 63|     Kevin|129632.55|             Georgia|\n",
      "| 64|    Dennis|280933.71|            Portugal|\n",
      "| 66|    Steven|152382.69|             Namibia|\n",
      "| 67|  Jonathan|268468.96|               China|\n",
      "| 68|    Rachel|234502.16|           Indonesia|\n",
      "| 69|    Harold|146917.43|               China|\n",
      "| 70|    Pamela|253108.75|               Italy|\n",
      "| 72|      John| 91566.02|              Sweden|\n",
      "| 74|   Kathryn| 59413.85|             Ukraine|\n",
      "| 75| Catherine| 92315.94|           Indonesia|\n",
      "| 76|   Carolyn| 179193.6|             Estonia|\n",
      "| 77|    Denise|121013.48|               China|\n",
      "| 78|   Mildred|166987.55|              Russia|\n",
      "| 79|     Linda| 67211.67|              Russia|\n",
      "| 80|      Anna|110408.87|           Indonesia|\n",
      "| 84|   Kathryn|131855.43|         Philippines|\n",
      "| 85|   Lillian|145282.64|              Russia|\n",
      "| 87|    Dennis| 265985.0|             Croatia|\n",
      "| 88|   Timothy|242129.05|             Tunisia|\n",
      "| 89|    Nicole|258772.36|           Indonesia|\n",
      "| 90|Jacqueline|100733.44|               China|\n",
      "| 92|    Donald|105051.77|           Indonesia|\n",
      "| 93| Katherine|155597.16|              Poland|\n",
      "| 94|      Ruth| 181481.5|           Indonesia|\n",
      "| 95|   Stephen| 83986.79|           Guatemala|\n",
      "| 96|     Kevin|130054.63|       United States|\n",
      "| 97|    Steven|238119.62|              France|\n",
      "| 98|     Shawn| 67749.83|           Indonesia|\n",
      "|100|    Willie|175694.61|              Mexico|\n",
      "|101|    Louise|241582.88|           Indonesia|\n",
      "|102|    Justin|177230.52|               China|\n",
      "|106|  Jonathan| 90686.59|       United States|\n",
      "|107|     Irene| 80083.39|              Russia|\n",
      "|108|  Lawrence|275527.02|             Lebanon|\n",
      "|110|   Kathryn|203281.91|            Ethiopia|\n",
      "|111|     Linda| 68571.67|              Mexico|\n",
      "|112|    Nicole|172847.04|            Portugal|\n",
      "|113|      Mary|266115.82|              France|\n",
      "|114|   Richard|193643.49|               China|\n",
      "|115|     Larry|122235.49|       United States|\n",
      "|117|      Mark|268876.47|            Thailand|\n",
      "|118|     Randy|130062.87|            Malaysia|\n",
      "|119|     Craig|259437.58|              Russia|\n",
      "|121|      Lori|249075.17|              France|\n",
      "|122|   Brandon|248316.41|      Norfolk Island|\n",
      "+---+----------+---------+--------------------+\n",
      "only showing top 100 rows\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table.query_table(['id','first_name','salary','country'], ['salary>50000'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fd8e02-52f5-48e9-ad92-cb73ee3c0f56",
   "metadata": {},
   "source": [
    "### Expose mode: CSV\n",
    "\n",
    "This expose mode exports the data into CSV format, we just need to run `expose_as(\"CSV\")`on the desired table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5f596f5-a5c0-4983-9dfa-50a390af7dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/10 13:44:17 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n",
      "22/07/10 13:44:17 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n",
      "22/07/10 13:44:17 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "22/07/10 13:44:17 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n",
      "22/07/10 13:44:17 INFO org.sparkproject.jetty.util.log: Logging initialized @4332ms to org.sparkproject.jetty.util.log.Slf4jLog\n",
      "22/07/10 13:44:17 INFO org.sparkproject.jetty.server.Server: jetty-9.4.40.v20210413; built: 2021-04-13T20:42:42.668Z; git: b881a572662e1943a14ae12e7e1207989f218b74; jvm 1.8.0_332-b09\n",
      "22/07/10 13:44:17 INFO org.sparkproject.jetty.server.Server: Started @4457ms\n",
      "22/07/10 13:44:17 INFO org.sparkproject.jetty.server.AbstractConnector: Started ServerConnector@61c69bf1{HTTP/1.1, (http/1.1)}{0.0.0.0:37473}\n",
      "22/07/10 13:44:18 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at spark-cluster-m/10.168.0.42:8032\n",
      "22/07/10 13:44:18 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at spark-cluster-m/10.168.0.42:10200\n",
      "22/07/10 13:44:19 INFO org.apache.hadoop.conf.Configuration: resource-types.xml not found\n",
      "22/07/10 13:44:19 INFO org.apache.hadoop.yarn.util.resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "22/07/10 13:44:21 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1657456539755_0013\n",
      "22/07/10 13:44:22 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at spark-cluster-m/10.168.0.42:8030\n",
      "22/07/10 13:44:24 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.\n",
      "POLYEXPOSE - Reading table: default.polytable03\n",
      "\n",
      "ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/etc/hive/conf.dist/ivysettings.xml will be used\n",
      "POLYEXPOSE - Data exposed as CSV in : gs://gcs-velascoluis-dev-sandbox/default.polytable03\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table.expose_as(\"CSV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33d75c9-42af-4096-87da-94b709c5caf4",
   "metadata": {},
   "source": [
    "Lets inspect the first bytes of the exported data in GCS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d868f88f-daf1-4621-8529-975b1b55115c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-02-03T07:55:29.000Z,1,Amanda,Jordan,ajordan0@com.com,Female,1.197.201.2,6759521864920116,Indonesia,3/8/1971,49756.53,Internal Auditor,1E+02\n",
      "2016-02-03T17:04:03.000Z,2,Albert,Freeman,afreeman1@is.gd,Male,218.111.175.34,\"\",Canada,1/16/1968,150280.17,Accountant IV,\"\"\n",
      "2016-02-03T01:09:31.000Z,3,Evelyn,Morgan,emorgan2@altervista.org,Female,7.161.136.94,6767119071901597,Russia,2/1/1960,144972.51,Structural Engineer,\"\"\n",
      "2016-02-03T00:36:21.000Z,4,Denise,Riley,driley3@gmpg.org,Female,140.35.109.83,35760315989656"
     ]
    }
   ],
   "source": [
    "!gsutil cat -r 0-512 gs://gcs-velascoluis-dev-sandbox/default.polytable03/*csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53859034-b0a6-4435-b272-ba9b37fba45d",
   "metadata": {},
   "source": [
    "### Expose mode: GQL\n",
    "\n",
    "This expose mode generates and API endpoint where random GQL commands can be issued agains the table. First we need to enable it using `expose_mode('GQL')` and then we can send GQL statements via the `exec_gql(gql_query)` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d9d7877-bd9d-43d6-ad61-755b39bb0c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/10 13:45:03 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n",
      "22/07/10 13:45:03 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n",
      "22/07/10 13:45:03 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "22/07/10 13:45:03 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n",
      "22/07/10 13:45:03 INFO org.sparkproject.jetty.util.log: Logging initialized @4171ms to org.sparkproject.jetty.util.log.Slf4jLog\n",
      "22/07/10 13:45:03 INFO org.sparkproject.jetty.server.Server: jetty-9.4.40.v20210413; built: 2021-04-13T20:42:42.668Z; git: b881a572662e1943a14ae12e7e1207989f218b74; jvm 1.8.0_332-b09\n",
      "22/07/10 13:45:03 INFO org.sparkproject.jetty.server.Server: Started @4299ms\n",
      "22/07/10 13:45:03 INFO org.sparkproject.jetty.server.AbstractConnector: Started ServerConnector@1d01f8d0{HTTP/1.1, (http/1.1)}{0.0.0.0:41147}\n",
      "22/07/10 13:45:04 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at spark-cluster-m/10.168.0.42:8032\n",
      "22/07/10 13:45:04 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at spark-cluster-m/10.168.0.42:10200\n",
      "22/07/10 13:45:05 INFO org.apache.hadoop.conf.Configuration: resource-types.xml not found\n",
      "22/07/10 13:45:05 INFO org.apache.hadoop.yarn.util.resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "22/07/10 13:45:06 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1657456539755_0014\n",
      "22/07/10 13:45:07 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at spark-cluster-m/10.168.0.42:8030\n",
      "22/07/10 13:45:10 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.\n",
      "POLYEXPOSE - Reading table: default.polytable03\n",
      "\n",
      "ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/etc/hive/conf.dist/ivysettings.xml will be used\n",
      "POLYEXPOSE - Data staged in : polyexpose.default_polytable03\n",
      "POLYEXPOSE - You can now issue GQL queries against polyexpose_default_polytable03\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table.expose_as(\"GQL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3feb8706-a14f-4d2f-8b9c-687e19f621bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"data\":{\"polyexpose_default_polytable03\":[{\"id\":\"157\",\"country\":\"France\",\"first_name\":\"\",\"salary\":\"155562.05\"},{\"id\":\"282\",\"country\":\"China\",\"first_name\":\"\",\"salary\":\"83888.12\"},{\"id\":\"404\",\"country\":\"Japan\",\"first_name\":\"\",\"salary\":\"227502.55\"},{\"id\":\"545\",\"country\":\"Japan\",\"first_name\":\"\",\"salary\":\"102158.99\"},{\"id\":\"653\",\"country\":\"China\",\"first_name\":\"\",\"salary\":\"127912.54\"},{\"id\":\"760\",\"country\":\"China\",\"first_name\":\"\",\"salary\":\"259094.47\"},{\"id\":\"865\",\"country\":\"Indonesia\",\"first_name\":\"\",\"salary\":\"92768.07\"},{\"id\":\"983\",\"country\":\"China\",\"first_name\":\"\",\"salary\":\"104324.94\"},{\"id\":\"195\",\"country\":\"Indonesia\",\"first_name\":\"Joe\",\"salary\":\"239690.34\"},{\"id\":\"618\",\"country\":\"Mexico\",\"first_name\":\"Joe\",\"salary\":\"273837.25\"}]}}\n"
     ]
    }
   ],
   "source": [
    "gql_query = \"query  { \\\n",
    "polyexpose_default_polytable03(limit: 10, where: {salary: {_gt: 50000}}) { \\\n",
    "    id \\\n",
    "    country \\\n",
    "    first_name \\\n",
    "    salary \\\n",
    "  } \\\n",
    "}\"\n",
    "table.exec_gql(gql_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7845939d-4b3d-4225-9002-d3bd0ca30b8c",
   "metadata": {},
   "source": [
    "### Expose mode: EVENTs\n",
    "\n",
    "This expose mode a pubsublite topic (KAFKA compatible) that can be read from a client suscription in real time. It used SPARK Structured Streaming. It follows the same convention, so it gets activated running `expose_as(\"EVENT\")` There are two key elements to configure at `config.yaml` time: `stream_time` and `timestamp_offset_mili` The first one controls the time the stream service will be running in seconds (e.g. 3600) During this time the dataproc SPARK cluster will be up listening for changes on the base Iceberg table. The second one is the offset time in miliseconds on where to look for table changes. It can 0 to listen for new changes or can be negative (e.g. -1000000000), to start generating events from that point in the past. All the data in the table is read and compacted into the `data`field of the  [pubsublite message](https://cloud.google.com/pubsub/lite/docs/reference/rpc/google.cloud.pubsublite.v1#pubsubmessage). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96afac16-47a2-49d1-b3d7-f6ccb4da6bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/10 13:46:27 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n",
      "22/07/10 13:46:27 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n",
      "22/07/10 13:46:27 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "22/07/10 13:46:27 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n",
      "22/07/10 13:46:27 INFO org.sparkproject.jetty.util.log: Logging initialized @4318ms to org.sparkproject.jetty.util.log.Slf4jLog\n",
      "22/07/10 13:46:27 INFO org.sparkproject.jetty.server.Server: jetty-9.4.40.v20210413; built: 2021-04-13T20:42:42.668Z; git: b881a572662e1943a14ae12e7e1207989f218b74; jvm 1.8.0_332-b09\n",
      "22/07/10 13:46:27 INFO org.sparkproject.jetty.server.Server: Started @4437ms\n",
      "22/07/10 13:46:27 INFO org.sparkproject.jetty.server.AbstractConnector: Started ServerConnector@5b3cdc9{HTTP/1.1, (http/1.1)}{0.0.0.0:43187}\n",
      "22/07/10 13:46:28 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at spark-cluster-m/10.168.0.42:8032\n",
      "22/07/10 13:46:28 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at spark-cluster-m/10.168.0.42:10200\n",
      "22/07/10 13:46:29 INFO org.apache.hadoop.conf.Configuration: resource-types.xml not found\n",
      "22/07/10 13:46:29 INFO org.apache.hadoop.yarn.util.resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "22/07/10 13:46:31 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1657456539755_0015\n",
      "22/07/10 13:46:32 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at spark-cluster-m/10.168.0.42:8030\n",
      "22/07/10 13:46:34 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.\n",
      "POLYEXPOSE - Reading table in stream mode: default.polytable03\n",
      "\n",
      "22/07/10 13:46:42 WARN org.apache.spark.sql.streaming.StreamingQueryManager: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "22/07/10 13:46:56 WARN org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 1000 milliseconds, but spent 12709 milliseconds\n",
      "POLYEXPOSE - Query executed, check subscription \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table.expose_as(\"EVENT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1269eefd-4fb6-4b77-a2fa-723f7c6ed4f7",
   "metadata": {},
   "source": [
    "Now client code, reading using a suscription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "01f7caf6-7b67-4e42-a01f-c68fdf6f5b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening for messages on projects/1063524325524/locations/us-west2/subscriptions/subscription_icebergtable...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from concurrent.futures._base import TimeoutError\n",
    "from google.pubsub_v1 import PubsubMessage\n",
    "from google.cloud.pubsublite.cloudpubsub import SubscriberClient\n",
    "from google.cloud.pubsublite.types import (\n",
    "    CloudRegion,\n",
    "    FlowControlSettings,\n",
    "    MessageMetadata,\n",
    "    SubscriptionPath,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "project_number = 1063524325524\n",
    "cloud_region = \"us-west2\"\n",
    "\n",
    "\n",
    "subscription_id = \"subscription_icebergtable\"\n",
    "timeout = 90\n",
    "location = CloudRegion(cloud_region)\n",
    "subscription_path = SubscriptionPath(project_number, location, subscription_id)\n",
    "per_partition_flow_control_settings = FlowControlSettings(\n",
    "    messages_outstanding=1000,\n",
    "    bytes_outstanding=10 * 1024 * 1024,\n",
    ")\n",
    "\n",
    "def callback(message: PubsubMessage):\n",
    "    message_data = message.data.decode(\"utf-8\")\n",
    "    metadata = MessageMetadata.decode(message.message_id)\n",
    "    print(\n",
    "        f\"Received {message_data} of ordering key {message.ordering_key} with id {metadata}.\"\n",
    "    )\n",
    "    message.ack()\n",
    "\n",
    "with SubscriberClient() as subscriber_client:\n",
    "\n",
    "    streaming_pull_future = subscriber_client.subscribe(\n",
    "        subscription_path,\n",
    "        callback=callback,\n",
    "        per_partition_flow_control_settings=per_partition_flow_control_settings,\n",
    "    )\n",
    "\n",
    "    print(f\"Listening for messages on {str(subscription_path)}...\")\n",
    "\n",
    "    try:\n",
    "        streaming_pull_future.result(timeout=timeout)\n",
    "    except TimeoutError or KeyboardInterrupt:\n",
    "        streaming_pull_future.cancel()\n",
    "        assert streaming_pull_future.done()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m94",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m94"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
