bigquery:
  #BigQuery dataset name for hasura staging
  dataset: <TO_DO_DEVELOPER>
  #BigQuery dataset location (e.g. US) for hasura staging
  dataset_location: <TO_DO_DEVELOPER>
gcp:
  #Google Cloud Project ID
  gcp_project_id: <TO_DO_DEVELOPER>
  #Google Project number
  gcp_project_number: <TO_DO_DEVELOPER>
  #Google default region
  gcp_region: <TO_DO_DEVELOPER>
  #Google default zone
  gcp_zone: <TO_DO_DEVELOPER>
  #General Service Account name, it needs enough privigies to create/edit resources .. (e.g. Project Owner)
  sa_name: <TO_DO_DEVELOPER>
  #General Service Account name key location where the JSON file will be generated
  sa_key: <TO_DO_DEVELOPER>
  #Terrafrom Service Account name, it needs enough privigies to create/edit resources ..
  sa_terraform_name: <TO_DO_DEVELOPER>
  #Terraform Service Account name key location where the JSON file will be generated
  sa_terraform_key: <TO_DO_DEVELOPER>
gke:
  #GKE cluster name where graphQL engine will be deployed
  gke_cluster_name: <TO_DO_DEVELOPER>
  #hasura needs a metadata database, this will be the prefix name of a Postgres 9.6 database
  dbname_prefix: <TO_DO_DEVELOPER>
gcs:
  #GCS bucket location
  bucket_location: <TO_DO_DEVELOPER>
  #GCS bucket name
  bucket_name: <TO_DO_DEVELOPER>
  #GCS path where sample data will be deployed (e.g. /stg_parquet/ )
  sample_data_folder: <TO_DO_DEVELOPER>
  #GCS path where the SPARK templates will be deployed (e.g. /scripts/ )
  scripts_folder: <TO_DO_DEVELOPER>
  #GCS path Iceberg SPARK catalog (e.g. /warehouse/)
  #TODO (velascoluis) : Add support for metastore service
  warehouse_dir_folder: <TO_DO_DEVELOPER>
hasura:
  #Leave this filed unchanged for now
  url: <UNKNOWN>
pubsublite:
  #PubSubLite Topic region, the name is autogenerated from the base table
  pubsublite_gcp_region: <TO_DO_DEVELOPER>
  #This is the time the stream service will be running in seconds (e.g. 3600) During this time the dataproc SPARK cluster will be up listening for changes on the base Iceberg table
  stream_time:
    <TO_DO_DEVELOPER>
    #This is the offset time in miliseconds on where to look for table changes. It can 0 to listen for new changes or can be negative (e.g. -1000000000), to start generating events from that point in the past
  timestamp_offset_mili: <TO_DO_DEVELOPER>
spark:
  #SPARK cluster name (single node cluster for now)
  dataproc_cluster_name: <TO_DO_DEVELOPER>
  #SPARK dataproc GCP region
  dataproc_gcp_region: <TO_DO_DEVELOPER>
  spark_templates:
    #Do not edit these fields
    - iceberg_show_catalog: show_catalog.py
    - iceberg_exec_sql: exec_sql.py
    - iceberg_export_csv: export_csv.py
    - iceberg_export_bigquery: export_bigquery.py
    - iceberg_stream_pubsub: stream_pubsub.py
    - create_sample_table: create_sample_table.py
